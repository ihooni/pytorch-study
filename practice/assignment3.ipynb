{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1FO0yEszdOTkpkhUsP9O_HYHhy2P_N2jS",
      "authorship_tag": "ABX9TyMtjr0GUiOWLgOj4WS0YX9m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qoxvIwvWSp3",
        "colab_type": "text"
      },
      "source": [
        "### 1. Load training/validation image dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LgtisFOWUQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "data_root = './data/horse-or-human'\n",
        "\n",
        "# define vectorize transformer\n",
        "class VectorizeTransform:\n",
        "    def __call__(self, img):\n",
        "        return torch.reshape(img, (-1, ))\n",
        "\n",
        "# compose image transformer\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    VectorizeTransform()    # for vectorizing input image\n",
        "])\n",
        "\n",
        "# load training dataset\n",
        "train_data_path = data_root + '/train'\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2048,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# load validation dataset\n",
        "valid_data_path = data_root + '/validation'\n",
        "valid_dataset = torchvision.datasets.ImageFolder(root=valid_data_path, transform=transform)\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=2048,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LOa9ccKZp5U",
        "colab_type": "text"
      },
      "source": [
        "### 2. Define model and functions for learning neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv72GeXqZyWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "def loss(y_pred, y):\n",
        "  epsilon = 1e-12\n",
        "  return -1 * torch.mean(\n",
        "      y * torch.log(y_pred + epsilon) + (1 - y) * torch.log(1 - y_pred + epsilon)\n",
        "  )\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "  answer = (y_pred >= 0.5).float()\n",
        "  return torch.mean((answer == y).float())\n",
        "\n",
        "class ThreeLayerNN:\n",
        "  # initialize parameters for neural network\n",
        "  def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate):\n",
        "    self.results = {}\n",
        "    self.params = {}\n",
        "\n",
        "    self.params['w1'] = math.sqrt(2 / (input_size + hidden1_size)) * torch.randn(hidden1_size, input_size)\n",
        "    self.params['b1'] = torch.zeros((1, hidden1_size))\n",
        "    \n",
        "    self.params['w2'] = math.sqrt(2 / (hidden1_size + hidden2_size)) * torch.randn(hidden2_size, hidden1_size)\n",
        "    self.params['b2'] = torch.zeros((1, hidden2_size))\n",
        "    \n",
        "    self.params['w3'] = math.sqrt(2 / (hidden2_size + output_size)) * torch.randn(output_size, hidden2_size)\n",
        "    self.params['b3'] = torch.zeros((1, output_size))\n",
        "\n",
        "    self.lr = learning_rate\n",
        "\n",
        "  # compute y prediction\n",
        "  def predict(self, x):\n",
        "    self.forward(x) # forward propagation\n",
        "    pred = self.results['a3']\n",
        "\n",
        "    return pred\n",
        "\n",
        "  # compute gradients of each parameters and run the gradient descent algorithm\n",
        "  def gradient_descent(self, x, y):\n",
        "    # fetch model parameters\n",
        "    w1, w2, w3 = self.params['w1'], self.params['w2'], self.params['w3']\n",
        "    b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
        "\n",
        "    self.forward(x) # forward propagation\n",
        "\n",
        "    a0 = x\n",
        "    # fetch forward propagation results\n",
        "    a1, a2, a3 = self.results['a1'], self.results['a2'], self.results['a3']\n",
        "    z1, z2, z3 = self.results['z1'], self.results['z2'], self.results['z3']\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # backward propagation\n",
        "    dz3 = (a3 - y) / batch_size\n",
        "    dw3 = torch.matmul(dz3.T, a2)\n",
        "    db3 = torch.sum(dz3, axis=0)\n",
        "\n",
        "    da2 = torch.matmul(dz3, w3)\n",
        "    dz2 = a2 * (1 - a2) * da2\n",
        "    dw2 = torch.matmul(dz2.T, a1)\n",
        "    db2 = torch.sum(dz2, axis=0)\n",
        "\n",
        "    da1 = torch.matmul(dz2, w2)\n",
        "    dz1 = a1 * (1 - a1) * da1\n",
        "    dw1 = torch.matmul(dz1.T, a0)\n",
        "    db1 = torch.sum(dz1, axis=0)\n",
        "\n",
        "    # update model parameters\n",
        "    self.params['w1'] = w1 - self.lr * dw1\n",
        "    self.params['b1'] = b1 - self.lr * db1\n",
        "    self.params['w2'] = w2 - self.lr * dw2\n",
        "    self.params['b2'] = b2 - self.lr * db2\n",
        "    self.params['w3'] = w3 - self.lr * dw3\n",
        "    self.params['b3'] = b3 - self.lr * db3\n",
        "\n",
        "  # run forward propagation\n",
        "  def forward(self, x):\n",
        "    # fetch model parameters\n",
        "    w1, w2, w3 = self.params['w1'], self.params['w2'], self.params['w3']\n",
        "    b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
        "\n",
        "    a0 = x\n",
        "\n",
        "    z1 = torch.matmul(a0, w1.T) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = torch.matmul(a1, w2.T) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    z3 = torch.matmul(a2, w3.T) + b3\n",
        "    a3 = sigmoid(z3)\n",
        "\n",
        "    # store intermediate forward propagation results\n",
        "    self.results['a1'], self.results['a2'], self.results['a3'] = a1, a2, a3\n",
        "    self.results['z1'], self.results['z2'], self.results['z3'] = z1, z2, z3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z59OiF5N64m",
        "colab_type": "text"
      },
      "source": [
        "### 3. Learning with the gradient descent algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdcjfxXkODpA",
        "colab_type": "code",
        "outputId": "1b2904eb-229c-4400-fc4f-85a4276d67a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "import random\n",
        "\n",
        "it = iter(train_loader)\n",
        "images, labels = it.next()\n",
        "\n",
        "train_data_count = len(train_loader.dataset)\n",
        "valid_data_count = len(valid_loader.dataset)\n",
        "\n",
        "# initialize parameters\n",
        "epoch_count = 3000\n",
        "learning_rate = 0.062\n",
        "# learning_rate = 10 ** random.uniform(-1, -0.22)\n",
        "print(\"%.16f\" % learning_rate)\n",
        "\n",
        "# create new neural network\n",
        "nn = ThreeLayerNN(images.shape[1], 200, 100, 1, learning_rate)\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "for epoch in range(epoch_count):\n",
        "  # initialize current loss and accuracy\n",
        "  mean_train_loss, mean_train_acc = 0, 0\n",
        "  mean_valid_loss, mean_valid_acc = 0, 0\n",
        "\n",
        "  # calculate train loss and accuracy at this epoch\n",
        "  for batch_idx, (x_train, y_train) in enumerate(train_loader):\n",
        "    batch_size = x_train.shape[0]\n",
        "\n",
        "    y_train = torch.reshape(y_train, (batch_size, 1))\n",
        "    y_pred_train = nn.predict(x_train)\n",
        " \n",
        "    mean_train_loss += (batch_size / train_data_count) * loss(y_pred_train, y_train)\n",
        "    mean_train_acc += (batch_size / train_data_count) * accuracy(y_pred_train, y_train)\n",
        "\n",
        "  # calculate validation loss and accuracy at this epoch\n",
        "  for batch_idx, (x_valid, y_valid) in enumerate(valid_loader):\n",
        "    batch_size = x_valid.shape[0]\n",
        "\n",
        "    y_valid = torch.reshape(y_valid, (batch_size, 1))\n",
        "    y_pred_valid = nn.predict(x_valid)\n",
        "\n",
        "    mean_valid_loss += (batch_size / valid_data_count) * loss(y_pred_valid, y_valid)\n",
        "    mean_valid_acc += (batch_size / valid_data_count) * accuracy(y_pred_valid, y_valid)\n",
        "\n",
        "  # run the gradient descent algorithm using train dataset at this epoch\n",
        "  for batch_idx, (x_train, y_train) in enumerate(train_loader):\n",
        "    batch_size = x_train.shape[0]\n",
        "\n",
        "    y_train = torch.reshape(y_train, (batch_size, 1))\n",
        "    nn.gradient_descent(x_train, y_train)\n",
        "\n",
        "  # save losses and accuracies at this epoch\n",
        "  train_losses.append(mean_train_loss)\n",
        "  train_accs.append(mean_train_acc)\n",
        "  valid_losses.append(mean_valid_loss)\n",
        "  valid_accs.append(mean_valid_acc)\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(f'epoch: {epoch}')\n",
        "    print(f'train loss: {mean_train_loss}')\n",
        "    print(f'validation loss: {mean_valid_loss}')\n",
        "    print(f'train accuracy: {mean_train_acc}')\n",
        "    print(f'validation accuracy: {mean_valid_acc}\\n\\n')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0620000000000000\n",
            "epoch: 0\n",
            "train loss: 0.6950848698616028\n",
            "validation loss: 0.6929339170455933\n",
            "train accuracy: 0.4868549108505249\n",
            "validation accuracy: 0.5\n",
            "\n",
            "\n",
            "epoch: 50\n",
            "train loss: 0.6522886753082275\n",
            "validation loss: 0.6471558809280396\n",
            "train accuracy: 0.6465433239936829\n",
            "validation accuracy: 0.5703125\n",
            "\n",
            "\n",
            "epoch: 100\n",
            "train loss: 0.6098660230636597\n",
            "validation loss: 0.5668262243270874\n",
            "train accuracy: 0.7312560677528381\n",
            "validation accuracy: 0.83984375\n",
            "\n",
            "\n",
            "epoch: 150\n",
            "train loss: 0.5571058392524719\n",
            "validation loss: 0.4701591432094574\n",
            "train accuracy: 0.7565725445747375\n",
            "validation accuracy: 0.8828125\n",
            "\n",
            "\n",
            "epoch: 200\n",
            "train loss: 0.5060631036758423\n",
            "validation loss: 0.39103788137435913\n",
            "train accuracy: 0.7760467529296875\n",
            "validation accuracy: 0.8671875\n",
            "\n",
            "\n",
            "epoch: 250\n",
            "train loss: 0.4629448652267456\n",
            "validation loss: 0.3412992060184479\n",
            "train accuracy: 0.7877312302589417\n",
            "validation accuracy: 0.86328125\n",
            "\n",
            "\n",
            "epoch: 300\n",
            "train loss: 0.4255797863006592\n",
            "validation loss: 0.31454727053642273\n",
            "train accuracy: 0.8091529011726379\n",
            "validation accuracy: 0.8671875\n",
            "\n",
            "\n",
            "epoch: 350\n",
            "train loss: 0.46723195910453796\n",
            "validation loss: 0.42882758378982544\n",
            "train accuracy: 0.7672833204269409\n",
            "validation accuracy: 0.77734375\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3CfEuwzh4UK",
        "colab_type": "text"
      },
      "source": [
        "### 4. Plot the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0j3XtG8iDCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the training and validatioㅡn loss at every epoch\n",
        "epoch_count = range(1, len(train_losses) + 1)\n",
        "plt.title('Loss at every iteration')\n",
        "plt.plot(epoch_count, train_losses, 'r-')\n",
        "plt.plot(epoch_count, valid_losses, 'b-')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hGGvhBziHrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the training and validation accuracy at every epoch\n",
        "epoch_count = range(1, len(train_accs) + 1)\n",
        "plt.title('Accuracy at every iteration')\n",
        "plt.plot(epoch_count, train_accs, 'r-')\n",
        "plt.plot(epoch_count, valid_accs, 'b-')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLOzAvcWiISC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# present the table for the final accuracy and loss with training and validation datasets\n",
        "fig = plt.figure(dpi=80)\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "table_data=[\n",
        "    [\"$\\\\bf{dataset}$\", \"$\\\\bf{loss}$\", \"$\\\\bf{accuracy}$\"],\n",
        "    [\"training\", train_losses[-1].item(), train_accs[-1].item()],\n",
        "    [\"validation\", valid_losses[-1].item(), valid_accs[-1].item()],\n",
        "]\n",
        "\n",
        "table = ax.table(cellText=table_data, loc='center', cellLoc='center')\n",
        "table.set_fontsize(14)\n",
        "table.scale(2,4)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}